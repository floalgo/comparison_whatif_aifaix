{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of What-if-tool and AIX/AIF 360\n",
    "\n",
    "**Content:**\n",
    "1. Introduction\n",
    "1. COMPAS Dataset\n",
    "1. Data Preprocessing\n",
    "1. What if Tool\n",
    "    * Logistic Regression\n",
    "    * DNN Classifier\n",
    "    * Examples\n",
    "1. AI Explainability & Fairness 360\n",
    "    1. AI Explainability 360\n",
    "        * Logistic Rule Regression\n",
    "        * Neural Net\n",
    "    2. AI Fairness 360\n",
    "1. Conclusion\n",
    "\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Machine learning is increasingly being deployed for decision-making and predictions within consequential domains that are affected by anti-discrimination laws due to the sensitive nature of the processed data These areas are for instance recruiting, criminal justice, credit scoring and education. The application of machine learning in these fields generally has the aim to make the respective prediction processes more objective as a substitute for human decision-making. However, the blind application of machine learning can even amplify existing biases in the historic training data. In case the data used for training the machine learning algorithm is biased, the resulting model will learn these biases and perpetuate discriminatory decisions against protected groups into the future. Hence there is a growing demand for explainable machine learning that detects and mitigates these biases.\n",
    "\n",
    "In this notebook we will investigate tools from Google (What-If Tool) and IBM (AIX360/AIF360), which enables you to explain your data and model. The experiments will be conducted on the COMPAS data set collected by ProPublica and will be based on existing walkthroughs of the official documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. COMPAS Dataset\n",
    "\n",
    "COMPAS stands for ‘Correctional Offender Management Profiling for Alternative Sanctions’ and is a risk assesment algorithm developed by Nortpoint,Inc. that incorporates the criminal records and demographic data to assess when a individuum is more likely to become a recidivist. Based on this score ProPublica collected data of all inmates from Broward County from 2013 to 2014 and released the [COMPAS dataset](https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis).\n",
    "\n",
    "In some states the COMPAS risk scores can be considered by judges during sentencing. The first time COMPAS was developed in 1998 and has been revised over the years as the knowledge base of criminology has grown and gets periodically updated. COMPAS is measuring both types of risk, dynamic risk(criminogenic risk) and static risk (historical risk). The risk assessment based on three types of scales.\n",
    "* **Pretrial Release Risk Scale:** current and pending charges, prior arrest history, previous pretrial failure, residential stability, employment status, community ties, and substance abuse are significant indicators affecting pretrial scores. \n",
    "* **General Recidivism Scale:** created to predict new criminal offenses after release of the prison. This scale use an individual criminal history. \n",
    "* **Violent Recidivism Scale:** this scale is designed to predict violent offenses based on their history of violence and non-compliance. \n",
    "\n",
    "The problem in this dataset is that African-American people pose a higher risk than caucasian people which are rated with a lower risk. The ProPublica investigation found that only 20% of the predicted recidivist actually commit a crime after release. Therefor we will examine the predictions of models trained on this skewed dataset with What-If Tool and AI Fairness & Explainability 360. The target variable is repeat offenders within two years of discharge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\florian\\anaconda3\\envs\\aix360\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\florian\\anaconda3\\envs\\aix360\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\florian\\anaconda3\\envs\\aix360\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\florian\\anaconda3\\envs\\aix360\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\florian\\anaconda3\\envs\\aix360\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\florian\\anaconda3\\envs\\aix360\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\users\\florian\\anaconda3\\envs\\aix360\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\florian\\anaconda3\\envs\\aix360\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\florian\\anaconda3\\envs\\aix360\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\florian\\anaconda3\\envs\\aix360\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\florian\\anaconda3\\envs\\aix360\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\florian\\anaconda3\\envs\\aix360\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# general\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# preprocessing\n",
    "from datetime import datetime\n",
    "\n",
    "# What if tool\n",
    "import tensorflow as tf\n",
    "import functools\n",
    "from witwidget.notebook.visualization import WitConfigBuilder\n",
    "from witwidget.notebook.visualization import WitWidget\n",
    "\n",
    "# AIX\n",
    "import torch\n",
    "from aix360.algorithms.protodash import ProtodashExplainer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# AIF\n",
    "from aif360.datasets import CompasDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.algorithms.preprocessing import Reweighing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Before Cleaning"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "      <th>compas_screening_date</th>\n",
       "      <th>sex</th>\n",
       "      <th>dob</th>\n",
       "      <th>age</th>\n",
       "      <th>age_cat</th>\n",
       "      <th>race</th>\n",
       "      <th>...</th>\n",
       "      <th>v_decile_score</th>\n",
       "      <th>v_score_text</th>\n",
       "      <th>v_screening_date</th>\n",
       "      <th>in_custody</th>\n",
       "      <th>out_custody</th>\n",
       "      <th>priors_count.1</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>event</th>\n",
       "      <th>two_year_recid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>miguel hernandez</td>\n",
       "      <td>miguel</td>\n",
       "      <td>hernandez</td>\n",
       "      <td>2013-08-14</td>\n",
       "      <td>Male</td>\n",
       "      <td>1947-04-18</td>\n",
       "      <td>69</td>\n",
       "      <td>Greater than 45</td>\n",
       "      <td>Other</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>Low</td>\n",
       "      <td>2013-08-14</td>\n",
       "      <td>2014-07-07</td>\n",
       "      <td>2014-07-14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>327</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>kevon dixon</td>\n",
       "      <td>kevon</td>\n",
       "      <td>dixon</td>\n",
       "      <td>2013-01-27</td>\n",
       "      <td>Male</td>\n",
       "      <td>1982-01-22</td>\n",
       "      <td>34</td>\n",
       "      <td>25 - 45</td>\n",
       "      <td>African-American</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>Low</td>\n",
       "      <td>2013-01-27</td>\n",
       "      <td>2013-01-26</td>\n",
       "      <td>2013-02-05</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>159</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>ed philo</td>\n",
       "      <td>ed</td>\n",
       "      <td>philo</td>\n",
       "      <td>2013-04-14</td>\n",
       "      <td>Male</td>\n",
       "      <td>1991-05-14</td>\n",
       "      <td>24</td>\n",
       "      <td>Less than 25</td>\n",
       "      <td>African-American</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>Low</td>\n",
       "      <td>2013-04-14</td>\n",
       "      <td>2013-06-16</td>\n",
       "      <td>2013-06-16</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>marcu brown</td>\n",
       "      <td>marcu</td>\n",
       "      <td>brown</td>\n",
       "      <td>2013-01-13</td>\n",
       "      <td>Male</td>\n",
       "      <td>1993-01-21</td>\n",
       "      <td>23</td>\n",
       "      <td>Less than 25</td>\n",
       "      <td>African-American</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>Medium</td>\n",
       "      <td>2013-01-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1174</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>bouthy pierrelouis</td>\n",
       "      <td>bouthy</td>\n",
       "      <td>pierrelouis</td>\n",
       "      <td>2013-03-26</td>\n",
       "      <td>Male</td>\n",
       "      <td>1973-01-22</td>\n",
       "      <td>43</td>\n",
       "      <td>25 - 45</td>\n",
       "      <td>Other</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>Low</td>\n",
       "      <td>2013-03-26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1102</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7209</th>\n",
       "      <td>10996</td>\n",
       "      <td>steven butler</td>\n",
       "      <td>steven</td>\n",
       "      <td>butler</td>\n",
       "      <td>2013-11-23</td>\n",
       "      <td>Male</td>\n",
       "      <td>1992-07-17</td>\n",
       "      <td>23</td>\n",
       "      <td>Less than 25</td>\n",
       "      <td>African-American</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>Medium</td>\n",
       "      <td>2013-11-23</td>\n",
       "      <td>2013-11-22</td>\n",
       "      <td>2013-11-24</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7210</th>\n",
       "      <td>10997</td>\n",
       "      <td>malcolm simmons</td>\n",
       "      <td>malcolm</td>\n",
       "      <td>simmons</td>\n",
       "      <td>2014-02-01</td>\n",
       "      <td>Male</td>\n",
       "      <td>1993-03-25</td>\n",
       "      <td>23</td>\n",
       "      <td>Less than 25</td>\n",
       "      <td>African-American</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>Medium</td>\n",
       "      <td>2014-02-01</td>\n",
       "      <td>2014-01-31</td>\n",
       "      <td>2014-02-02</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>790</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7211</th>\n",
       "      <td>10999</td>\n",
       "      <td>winston gregory</td>\n",
       "      <td>winston</td>\n",
       "      <td>gregory</td>\n",
       "      <td>2014-01-14</td>\n",
       "      <td>Male</td>\n",
       "      <td>1958-10-01</td>\n",
       "      <td>57</td>\n",
       "      <td>Greater than 45</td>\n",
       "      <td>Other</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>Low</td>\n",
       "      <td>2014-01-14</td>\n",
       "      <td>2014-01-13</td>\n",
       "      <td>2014-01-14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>808</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7212</th>\n",
       "      <td>11000</td>\n",
       "      <td>farrah jean</td>\n",
       "      <td>farrah</td>\n",
       "      <td>jean</td>\n",
       "      <td>2014-03-09</td>\n",
       "      <td>Female</td>\n",
       "      <td>1982-11-17</td>\n",
       "      <td>33</td>\n",
       "      <td>25 - 45</td>\n",
       "      <td>African-American</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>Low</td>\n",
       "      <td>2014-03-09</td>\n",
       "      <td>2014-03-08</td>\n",
       "      <td>2014-03-09</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>754</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7213</th>\n",
       "      <td>11001</td>\n",
       "      <td>florencia sanmartin</td>\n",
       "      <td>florencia</td>\n",
       "      <td>sanmartin</td>\n",
       "      <td>2014-06-30</td>\n",
       "      <td>Female</td>\n",
       "      <td>1992-12-18</td>\n",
       "      <td>23</td>\n",
       "      <td>Less than 25</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>Low</td>\n",
       "      <td>2014-06-30</td>\n",
       "      <td>2015-03-15</td>\n",
       "      <td>2015-03-15</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>258</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7214 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                 name      first         last  \\\n",
       "0         1     miguel hernandez     miguel    hernandez   \n",
       "1         3          kevon dixon      kevon        dixon   \n",
       "2         4             ed philo         ed        philo   \n",
       "3         5          marcu brown      marcu        brown   \n",
       "4         6   bouthy pierrelouis     bouthy  pierrelouis   \n",
       "...     ...                  ...        ...          ...   \n",
       "7209  10996        steven butler     steven       butler   \n",
       "7210  10997      malcolm simmons    malcolm      simmons   \n",
       "7211  10999      winston gregory    winston      gregory   \n",
       "7212  11000          farrah jean     farrah         jean   \n",
       "7213  11001  florencia sanmartin  florencia    sanmartin   \n",
       "\n",
       "     compas_screening_date     sex         dob  age          age_cat  \\\n",
       "0               2013-08-14    Male  1947-04-18   69  Greater than 45   \n",
       "1               2013-01-27    Male  1982-01-22   34          25 - 45   \n",
       "2               2013-04-14    Male  1991-05-14   24     Less than 25   \n",
       "3               2013-01-13    Male  1993-01-21   23     Less than 25   \n",
       "4               2013-03-26    Male  1973-01-22   43          25 - 45   \n",
       "...                    ...     ...         ...  ...              ...   \n",
       "7209            2013-11-23    Male  1992-07-17   23     Less than 25   \n",
       "7210            2014-02-01    Male  1993-03-25   23     Less than 25   \n",
       "7211            2014-01-14    Male  1958-10-01   57  Greater than 45   \n",
       "7212            2014-03-09  Female  1982-11-17   33          25 - 45   \n",
       "7213            2014-06-30  Female  1992-12-18   23     Less than 25   \n",
       "\n",
       "                  race  ...  v_decile_score  v_score_text  v_screening_date  \\\n",
       "0                Other  ...               1           Low        2013-08-14   \n",
       "1     African-American  ...               1           Low        2013-01-27   \n",
       "2     African-American  ...               3           Low        2013-04-14   \n",
       "3     African-American  ...               6        Medium        2013-01-13   \n",
       "4                Other  ...               1           Low        2013-03-26   \n",
       "...                ...  ...             ...           ...               ...   \n",
       "7209  African-American  ...               5        Medium        2013-11-23   \n",
       "7210  African-American  ...               5        Medium        2014-02-01   \n",
       "7211             Other  ...               1           Low        2014-01-14   \n",
       "7212  African-American  ...               2           Low        2014-03-09   \n",
       "7213          Hispanic  ...               4           Low        2014-06-30   \n",
       "\n",
       "      in_custody  out_custody  priors_count.1 start   end event two_year_recid  \n",
       "0     2014-07-07   2014-07-14               0     0   327     0              0  \n",
       "1     2013-01-26   2013-02-05               0     9   159     1              1  \n",
       "2     2013-06-16   2013-06-16               4     0    63     0              1  \n",
       "3            NaN          NaN               1     0  1174     0              0  \n",
       "4            NaN          NaN               2     0  1102     0              0  \n",
       "...          ...          ...             ...   ...   ...   ...            ...  \n",
       "7209  2013-11-22   2013-11-24               0     1   860     0              0  \n",
       "7210  2014-01-31   2014-02-02               0     1   790     0              0  \n",
       "7211  2014-01-13   2014-01-14               0     0   808     0              0  \n",
       "7212  2014-03-08   2014-03-09               3     0   754     0              0  \n",
       "7213  2015-03-15   2015-03-15               2     0   258     0              1  \n",
       "\n",
       "[7214 rows x 53 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load COMPAS dataset\n",
    "dataset = pd.read_csv(\"./data/compas-scores-two-years.csv\")\n",
    "display(Markdown(\"#### Before Cleaning\"))\n",
    "display(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running our models on the dataset we preprocess our data. Not all fields are relevant and we end up with following fields for our analysis.\n",
    "\n",
    "| Field                   | Description                             | Attributes                             |\n",
    "| ------------------------|:---------------------------------------:| --------------------------------------:|\n",
    "| age                     | Age of detainee                         | positive integer                       |\n",
    "| c_charge_degree         | crime degree                            | F=Felony, M=Misdemeanors               |\n",
    "| race                    | Ethnic group                            | African-American, Caucausian, ...         |\n",
    "| age_cat                 | Age categorized                         | age < 25, 25 <= age > 45 and 45 <= age |\n",
    "| score_text              | Risk of Recidivism categorized          | Low (1-3), Medium (4-7) or High (8-10) |\n",
    "| sex                     | Gender                                  | Female or Male                         |\n",
    "| priors_count            | Number of prior criminal records        | positive integer                       |\n",
    "| days_b_screening_arrest | Difference between charge and crime date | Integer                                |\n",
    "| decile_score            | Risk of Recidivism                      | 1-10                                   |\n",
    "| two_year_recid          | Criminal recidivism within two years after release | True/False                             | \n",
    "| c_jail_in               | Date of imprisonment                    | Date                                   |\n",
    "| c_jail_out              | Date of Release                         | Date                                   |\n",
    "\n",
    "Out **target** variable is 'two year recid', which is True if there is a criminal recidivism within two years and else False. Due to missing data we also perform some data cleaning to our dataset:\n",
    "* We simplify the time in jail of the detainee in a new attribute 'length_of_stay'.\n",
    "* We filter samples with 'days_b_screening_arrest' lesser or greater than 30. There is a high possibility that we don't have the right offense if the difference between charge date and crime date is bigger than 30 days.\n",
    "* Offenses with c_charge_degree = 0 (e.g. traffic offense) are not relevant and filtered out.\n",
    "* We filter samples with missing input in the field 'score_text'.\n",
    "* We filter samples that have a negative integer in the field 'length_of_stay'.\n",
    "\n",
    "After filtering we end up with approximately 1200 samples less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "jupyter_feat = [ 'age', 'c_charge_degree', 'race', 'age_cat', 'score_text', 'sex', 'priors_count', \n",
    "                    'days_b_screening_arrest', 'decile_score', 'two_year_recid', 'c_jail_in', 'c_jail_out']\n",
    "dataset = dataset.filter(items=jupyter_feat)\n",
    "\n",
    "# transformation\n",
    "dataset['length_of_stay'] = (pd.to_datetime(dataset['c_jail_out']) - pd.to_datetime(dataset['c_jail_in'])).dt.days\n",
    "\n",
    "# filter\n",
    "dataset = dataset.loc[dataset['days_b_screening_arrest'] <= 30]\n",
    "dataset = dataset.loc[dataset['days_b_screening_arrest'] >= -30]\n",
    "# dataset = dataset.loc[dataset['is_recid'] != -1]\n",
    "dataset = dataset.loc[dataset['c_charge_degree'] != 'O']\n",
    "dataset = dataset.loc[dataset['score_text'] != 'N/A']\n",
    "dataset = dataset.loc[dataset['length_of_stay'] >= 0] # Dauer im Gefängnis kann nicht negativ sein\n",
    "\n",
    "# type conversion\n",
    "dataset['length_of_stay'] = dataset['length_of_stay'].astype('int64')\n",
    "\n",
    "# drop irrelevant columns\n",
    "dataset.drop(columns=['c_jail_in', 'c_jail_out'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### After Cleaning"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 5989 samples and 11 features after cleaning\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>c_charge_degree</th>\n",
       "      <th>race</th>\n",
       "      <th>age_cat</th>\n",
       "      <th>score_text</th>\n",
       "      <th>sex</th>\n",
       "      <th>priors_count</th>\n",
       "      <th>days_b_screening_arrest</th>\n",
       "      <th>decile_score</th>\n",
       "      <th>two_year_recid</th>\n",
       "      <th>length_of_stay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69</td>\n",
       "      <td>F</td>\n",
       "      <td>Other</td>\n",
       "      <td>Greater than 45</td>\n",
       "      <td>Low</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34</td>\n",
       "      <td>F</td>\n",
       "      <td>African-American</td>\n",
       "      <td>25 - 45</td>\n",
       "      <td>Low</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24</td>\n",
       "      <td>F</td>\n",
       "      <td>African-American</td>\n",
       "      <td>Less than 25</td>\n",
       "      <td>Low</td>\n",
       "      <td>Male</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>44</td>\n",
       "      <td>M</td>\n",
       "      <td>Other</td>\n",
       "      <td>25 - 45</td>\n",
       "      <td>Low</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>41</td>\n",
       "      <td>F</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>25 - 45</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Male</td>\n",
       "      <td>14</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7209</th>\n",
       "      <td>23</td>\n",
       "      <td>F</td>\n",
       "      <td>African-American</td>\n",
       "      <td>Less than 25</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7210</th>\n",
       "      <td>23</td>\n",
       "      <td>F</td>\n",
       "      <td>African-American</td>\n",
       "      <td>Less than 25</td>\n",
       "      <td>Low</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7211</th>\n",
       "      <td>57</td>\n",
       "      <td>F</td>\n",
       "      <td>Other</td>\n",
       "      <td>Greater than 45</td>\n",
       "      <td>Low</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7212</th>\n",
       "      <td>33</td>\n",
       "      <td>M</td>\n",
       "      <td>African-American</td>\n",
       "      <td>25 - 45</td>\n",
       "      <td>Low</td>\n",
       "      <td>Female</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7213</th>\n",
       "      <td>23</td>\n",
       "      <td>F</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>Less than 25</td>\n",
       "      <td>Low</td>\n",
       "      <td>Female</td>\n",
       "      <td>2</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5989 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age c_charge_degree              race          age_cat score_text  \\\n",
       "0      69               F             Other  Greater than 45        Low   \n",
       "1      34               F  African-American          25 - 45        Low   \n",
       "2      24               F  African-American     Less than 25        Low   \n",
       "5      44               M             Other          25 - 45        Low   \n",
       "6      41               F         Caucasian          25 - 45     Medium   \n",
       "...   ...             ...               ...              ...        ...   \n",
       "7209   23               F  African-American     Less than 25     Medium   \n",
       "7210   23               F  African-American     Less than 25        Low   \n",
       "7211   57               F             Other  Greater than 45        Low   \n",
       "7212   33               M  African-American          25 - 45        Low   \n",
       "7213   23               F          Hispanic     Less than 25        Low   \n",
       "\n",
       "         sex  priors_count  days_b_screening_arrest  decile_score  \\\n",
       "0       Male             0                     -1.0             1   \n",
       "1       Male             0                     -1.0             3   \n",
       "2       Male             4                     -1.0             4   \n",
       "5       Male             0                      0.0             1   \n",
       "6       Male            14                     -1.0             6   \n",
       "...      ...           ...                      ...           ...   \n",
       "7209    Male             0                     -1.0             7   \n",
       "7210    Male             0                     -1.0             3   \n",
       "7211    Male             0                     -1.0             1   \n",
       "7212  Female             3                     -1.0             2   \n",
       "7213  Female             2                     -2.0             4   \n",
       "\n",
       "      two_year_recid  length_of_stay  \n",
       "0                  0               0  \n",
       "1                  1              10  \n",
       "2                  1               1  \n",
       "5                  0               1  \n",
       "6                  1               6  \n",
       "...              ...             ...  \n",
       "7209               0               1  \n",
       "7210               0               1  \n",
       "7211               0               1  \n",
       "7212               0               1  \n",
       "7213               1               1  \n",
       "\n",
       "[5989 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(\"#### After Cleaning\"))\n",
    "print(\"Dataset has %i samples and %i features after cleaning\" % (dataset.shape[0], dataset.shape[1]))\n",
    "display(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Distribution of sensitive features <br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Age:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counts</th>\n",
       "      <th>%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25 - 45</th>\n",
       "      <td>3424</td>\n",
       "      <td>57.171481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Less than 25</th>\n",
       "      <td>1312</td>\n",
       "      <td>21.906829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Greater than 45</th>\n",
       "      <td>1253</td>\n",
       "      <td>20.921690</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 counts          %\n",
       "25 - 45            3424  57.171481\n",
       "Less than 25       1312  21.906829\n",
       "Greater than 45    1253  20.921690"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "***"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Gender:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counts</th>\n",
       "      <th>%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Male</th>\n",
       "      <td>4849</td>\n",
       "      <td>80.965103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Female</th>\n",
       "      <td>1140</td>\n",
       "      <td>19.034897</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        counts          %\n",
       "Male      4849  80.965103\n",
       "Female    1140  19.034897"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "***"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Race:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counts</th>\n",
       "      <th>%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>African-American</th>\n",
       "      <td>3086</td>\n",
       "      <td>51.527801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Caucasian</th>\n",
       "      <td>2032</td>\n",
       "      <td>33.928870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hispanic</th>\n",
       "      <td>496</td>\n",
       "      <td>8.281850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other</th>\n",
       "      <td>333</td>\n",
       "      <td>5.560194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Asian</th>\n",
       "      <td>31</td>\n",
       "      <td>0.517616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Native American</th>\n",
       "      <td>11</td>\n",
       "      <td>0.183670</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  counts          %\n",
       "African-American    3086  51.527801\n",
       "Caucasian           2032  33.928870\n",
       "Hispanic             496   8.281850\n",
       "Other                333   5.560194\n",
       "Asian                 31   0.517616\n",
       "Native American       11   0.183670"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "***"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Race (only males):**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counts</th>\n",
       "      <th>%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>African-American</th>\n",
       "      <td>2558</td>\n",
       "      <td>52.753145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Caucasian</th>\n",
       "      <td>1562</td>\n",
       "      <td>32.212827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hispanic</th>\n",
       "      <td>415</td>\n",
       "      <td>8.558466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other</th>\n",
       "      <td>276</td>\n",
       "      <td>5.691895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Asian</th>\n",
       "      <td>29</td>\n",
       "      <td>0.598061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Native American</th>\n",
       "      <td>9</td>\n",
       "      <td>0.185605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  counts          %\n",
       "African-American    2558  52.753145\n",
       "Caucasian           1562  32.212827\n",
       "Hispanic             415   8.558466\n",
       "Other                276   5.691895\n",
       "Asian                 29   0.598061\n",
       "Native American        9   0.185605"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "***"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Race (only females):**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counts</th>\n",
       "      <th>%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>African-American</th>\n",
       "      <td>528</td>\n",
       "      <td>46.315789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Caucasian</th>\n",
       "      <td>470</td>\n",
       "      <td>41.228070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hispanic</th>\n",
       "      <td>81</td>\n",
       "      <td>7.105263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other</th>\n",
       "      <td>57</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Native American</th>\n",
       "      <td>2</td>\n",
       "      <td>0.175439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Asian</th>\n",
       "      <td>2</td>\n",
       "      <td>0.175439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  counts          %\n",
       "African-American     528  46.315789\n",
       "Caucasian            470  41.228070\n",
       "Hispanic              81   7.105263\n",
       "Other                 57   5.000000\n",
       "Native American        2   0.175439\n",
       "Asian                  2   0.175439"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "***"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show some distribution stats\n",
    "\n",
    "c = dataset['age_cat'].value_counts(dropna=False)\n",
    "p = dataset['age_cat'].value_counts(dropna=False, normalize=True) * 100\n",
    "age_result = pd.concat([c,p], axis=1, keys=['counts', '%'])\n",
    "\n",
    "c = dataset['sex'].value_counts(dropna=False)\n",
    "p = dataset['sex'].value_counts(dropna=False, normalize=True) * 100\n",
    "sex_result = pd.concat([c,p], axis=1, keys=['counts', '%'])\n",
    "\n",
    "c = dataset['race'].value_counts(dropna=False)\n",
    "p = dataset['race'].value_counts(dropna=False, normalize=True) * 100\n",
    "race_result = pd.concat([c,p], axis=1, keys=['counts', '%'])\n",
    "\n",
    "males = dataset.loc[dataset['sex'] == 'Male']\n",
    "c = males['race'].value_counts(dropna=False)\n",
    "p = males['race'].value_counts(dropna=False, normalize=True) * 100\n",
    "males_race_result = pd.concat([c,p], axis=1, keys=['counts', '%'])\n",
    "\n",
    "females = dataset.loc[dataset['sex'] == 'Female']\n",
    "c = females['race'].value_counts(dropna=False)\n",
    "p = females['race'].value_counts(dropna=False, normalize=True) * 100\n",
    "females_race_result = pd.concat([c,p], axis=1, keys=['counts', '%'])\n",
    "\n",
    "display(Markdown(\"#### Distribution of sensitive features <br><br>\"))\n",
    "display(Markdown(\"**Age:**\"), age_result, Markdown(\"***\"))\n",
    "display(Markdown(\"**Gender:**\"), sex_result, Markdown(\"***\"))\n",
    "display(Markdown(\"**Race:**\"), race_result, Markdown(\"***\"))\n",
    "display(Markdown(\"**Race (only males):**\"), males_race_result, Markdown(\"***\"))\n",
    "display(Markdown(\"**Race (only females):**\"), females_race_result, Markdown(\"***\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. What-If Tool (WIT)\n",
    "\n",
    "\n",
    "WIT is an application that enables you to investigate machine learning models with minimal coding. It supports Tensorflow and Python-accessible models. Additional you can use it via tensorboard or Jupyter/Colaboratory notebooks. It provides an easy to use interface with various algorithms to visualize, compare and adjust datapoints or models.\n",
    "\n",
    "In this notebook we embedded What-If tool via the pip package [witwidget](https://pypi.org/project/witwidget/). There are other options to set up WIT (e.g. docker, tensorboard, etc.) and even Google Cloud supports this addon. You can find out more about this and also different examples/walkthroughs [here](https://pair-code.github.io/what-if-tool/).\n",
    "\n",
    "First up we split the data in train and test and define some helper methods. After this we run a logistic regression and a DNN Classifier on our dataset and compare them with the WIT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split in train and test\n",
    "x_test = dataset.sample(frac=1/5)\n",
    "index_mask = dataset.index.isin(x_test.index)\n",
    "index_mask = [not i for i in index_mask]\n",
    "x_train = dataset[index_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Several helper methods copied from the official Income classification example\n",
    "# https://colab.research.google.com/github/pair-code/what-if-tool/blob/master/WIT_Model_Comparison.ipynb\n",
    "\n",
    "def df_to_examples(df, columns=None):\n",
    "    examples = []\n",
    "    if columns == None:\n",
    "        columns = df.columns.values.tolist()\n",
    "    for index, row in df.iterrows():\n",
    "        example = tf.train.Example()\n",
    "        for col in columns:\n",
    "            if df[col].dtype is np.dtype(np.int64):\n",
    "                example.features.feature[col].int64_list.value.append(int(row[col]))\n",
    "            elif df[col].dtype is np.dtype(np.float64):\n",
    "                example.features.feature[col].float_list.value.append(row[col])\n",
    "            elif row[col] == row[col]:\n",
    "                example.features.feature[col].bytes_list.value.append(str(row[col]).encode('utf-8'))\n",
    "        examples.append(example)\n",
    "    return examples\n",
    "\n",
    "# An input function for providing input to a model from tf.Examples\n",
    "def tfexamples_input_fn(examples, feature_spec, label, mode=tf.estimator.ModeKeys.EVAL,\n",
    "                       num_epochs=None, \n",
    "                       batch_size=64):\n",
    "    def ex_generator():\n",
    "        for i in range(len(examples)):\n",
    "            yield examples[i].SerializeToString()\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "      ex_generator, tf.dtypes.string, tf.TensorShape([]))\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        dataset = dataset.shuffle(buffer_size=2 * batch_size + 1)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(lambda tf_example: parse_tf_example(tf_example, label, feature_spec))\n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    return dataset\n",
    "\n",
    "# Parses Tf.Example protos into features for the input function.\n",
    "def parse_tf_example(example_proto, label, feature_spec):\n",
    "    parsed_features = tf.parse_example(serialized=example_proto, features=feature_spec)\n",
    "    target = parsed_features.pop(label)\n",
    "    return parsed_features, target\n",
    "\n",
    "def create_feature_spec(df, columns=None):\n",
    "    feature_spec = {}\n",
    "    if columns == None:\n",
    "        columns = df.columns.values.tolist()\n",
    "    for f in columns:\n",
    "        if df[f].dtype is np.dtype(np.int64):\n",
    "            feature_spec[f] = tf.FixedLenFeature(shape=(), dtype=tf.int64)\n",
    "        elif df[f].dtype is np.dtype(np.float64):\n",
    "            feature_spec[f] = tf.FixedLenFeature(shape=(), dtype=tf.float32)\n",
    "        else:\n",
    "            feature_spec[f] = tf.FixedLenFeature(shape=(), dtype=tf.string)\n",
    "    return feature_spec\n",
    "\n",
    "def create_feature_columns(columns, feature_spec):\n",
    "    ret = []\n",
    "    for col in columns:\n",
    "        if feature_spec[col].dtype is tf.int64 or feature_spec[col].dtype is tf.float32:\n",
    "            ret.append(tf.feature_column.numeric_column(col))\n",
    "        else:\n",
    "            ret.append(tf.feature_column.indicator_column(\n",
    "                tf.feature_column.categorical_column_with_vocabulary_list(col, list(dataset[col].unique()))))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the column in the dataset you wish for the model to predict\n",
    "label_column = 'two_year_recid'\n",
    "\n",
    "input_features = ['age', 'c_charge_degree', 'race', 'age_cat', 'score_text', 'sex', 'priors_count', \n",
    "                    'days_b_screening_arrest', 'decile_score', 'length_of_stay']\n",
    "\n",
    "# Create a list containing all input features and the label column\n",
    "features_and_labels = input_features + [label_column]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression & DNN Classifier\n",
    "\n",
    "We first define our linear classifier (logistic regression) and our non-linear classifier (DNN). We train both models on our training data and evaluate them with WIT on our testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\Florian\\AppData\\Local\\Temp\\tmpb7bbyi_y\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Users\\\\Florian\\\\AppData\\\\Local\\\\Temp\\\\tmpb7bbyi_y', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000017502CCDCF8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "WARNING:tensorflow:From c:\\users\\florian\\anaconda3\\envs\\aix360\\lib\\site-packages\\tensorflow\\python\\training\\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "WARNING:tensorflow:From c:\\users\\florian\\anaconda3\\envs\\aix360\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:494: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Entity <bound method LinearModel.call of <tensorflow.python.feature_column.feature_column_v2.LinearModel object at 0x0000017502F60F28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LinearModel.call of <tensorflow.python.feature_column.feature_column_v2.LinearModel object at 0x0000017502F60F28>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method LinearModel.call of <tensorflow.python.feature_column.feature_column_v2.LinearModel object at 0x0000017502F60F28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LinearModel.call of <tensorflow.python.feature_column.feature_column_v2.LinearModel object at 0x0000017502F60F28>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method _LinearModelLayer.call of <tensorflow.python.feature_column.feature_column_v2._LinearModelLayer object at 0x000001750274A5F8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method _LinearModelLayer.call of <tensorflow.python.feature_column.feature_column_v2._LinearModelLayer object at 0x000001750274A5F8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method _LinearModelLayer.call of <tensorflow.python.feature_column.feature_column_v2._LinearModelLayer object at 0x000001750274A5F8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method _LinearModelLayer.call of <tensorflow.python.feature_column.feature_column_v2._LinearModelLayer object at 0x000001750274A5F8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "DEBUG:tensorflow:Transforming feature_column NumericColumn(key='age', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None).\n",
      "DEBUG:tensorflow:Transforming feature_column IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='age_cat', vocabulary_list=('Greater than 45', '25 - 45', 'Less than 25'), dtype=tf.string, default_value=-1, num_oov_buckets=0)).\n",
      "DEBUG:tensorflow:Transforming feature_column VocabularyListCategoricalColumn(key='age_cat', vocabulary_list=('Greater than 45', '25 - 45', 'Less than 25'), dtype=tf.string, default_value=-1, num_oov_buckets=0).\n",
      "WARNING:tensorflow:From c:\\users\\florian\\anaconda3\\envs\\aix360\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column_v2.py:2655: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From c:\\users\\florian\\anaconda3\\envs\\aix360\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column_v2.py:4207: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From c:\\users\\florian\\anaconda3\\envs\\aix360\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column_v2.py:4262: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "DEBUG:tensorflow:Transforming feature_column IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='c_charge_degree', vocabulary_list=('F', 'M'), dtype=tf.string, default_value=-1, num_oov_buckets=0)).\n",
      "DEBUG:tensorflow:Transforming feature_column VocabularyListCategoricalColumn(key='c_charge_degree', vocabulary_list=('F', 'M'), dtype=tf.string, default_value=-1, num_oov_buckets=0).\n",
      "DEBUG:tensorflow:Transforming feature_column NumericColumn(key='days_b_screening_arrest', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None).\n",
      "DEBUG:tensorflow:Transforming feature_column NumericColumn(key='decile_score', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None).\n",
      "DEBUG:tensorflow:Transforming feature_column NumericColumn(key='length_of_stay', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None).\n",
      "DEBUG:tensorflow:Transforming feature_column NumericColumn(key='priors_count', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None).\n",
      "DEBUG:tensorflow:Transforming feature_column IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='race', vocabulary_list=('Other', 'African-American', 'Caucasian', 'Hispanic', 'Asian', 'Native American'), dtype=tf.string, default_value=-1, num_oov_buckets=0)).\n",
      "DEBUG:tensorflow:Transforming feature_column VocabularyListCategoricalColumn(key='race', vocabulary_list=('Other', 'African-American', 'Caucasian', 'Hispanic', 'Asian', 'Native American'), dtype=tf.string, default_value=-1, num_oov_buckets=0).\n",
      "DEBUG:tensorflow:Transforming feature_column IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='score_text', vocabulary_list=('Low', 'Medium', 'High'), dtype=tf.string, default_value=-1, num_oov_buckets=0)).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:tensorflow:Transforming feature_column VocabularyListCategoricalColumn(key='score_text', vocabulary_list=('Low', 'Medium', 'High'), dtype=tf.string, default_value=-1, num_oov_buckets=0).\n",
      "DEBUG:tensorflow:Transforming feature_column IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='sex', vocabulary_list=('Male', 'Female'), dtype=tf.string, default_value=-1, num_oov_buckets=0)).\n",
      "DEBUG:tensorflow:Transforming feature_column VocabularyListCategoricalColumn(key='sex', vocabulary_list=('Male', 'Female'), dtype=tf.string, default_value=-1, num_oov_buckets=0).\n",
      "WARNING:tensorflow:From c:\\users\\florian\\anaconda3\\envs\\aix360\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\canned\\linear.py:308: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into C:\\Users\\Florian\\AppData\\Local\\Temp\\tmpb7bbyi_y\\model.ckpt.\n",
      "INFO:tensorflow:loss = 44.36142, step = 1\n",
      "INFO:tensorflow:global_step/sec: 69.7373\n",
      "INFO:tensorflow:loss = 28.310158, step = 101 (1.436 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.4505\n",
      "INFO:tensorflow:loss = 37.24495, step = 201 (1.170 sec)\n",
      "INFO:tensorflow:global_step/sec: 62.5\n",
      "INFO:tensorflow:loss = 31.249006, step = 301 (1.600 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.8016\n",
      "INFO:tensorflow:loss = 39.790695, step = 401 (2.513 sec)\n",
      "INFO:tensorflow:global_step/sec: 53.562\n",
      "INFO:tensorflow:loss = 42.97735, step = 501 (1.867 sec)\n",
      "INFO:tensorflow:global_step/sec: 56.5456\n",
      "INFO:tensorflow:loss = 35.72177, step = 601 (1.769 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.9576\n",
      "INFO:tensorflow:loss = 38.00652, step = 701 (2.327 sec)\n",
      "INFO:tensorflow:global_step/sec: 44.0141\n",
      "INFO:tensorflow:loss = 38.644775, step = 801 (2.274 sec)\n",
      "INFO:tensorflow:global_step/sec: 54.5693\n",
      "INFO:tensorflow:loss = 32.774647, step = 901 (1.832 sec)\n",
      "INFO:tensorflow:global_step/sec: 58.2352\n",
      "INFO:tensorflow:loss = 35.780983, step = 1001 (1.716 sec)\n",
      "INFO:tensorflow:global_step/sec: 54.8228\n",
      "INFO:tensorflow:loss = 40.056114, step = 1101 (1.828 sec)\n",
      "INFO:tensorflow:global_step/sec: 72.9894\n",
      "INFO:tensorflow:loss = 37.832047, step = 1201 (1.369 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.2396\n",
      "INFO:tensorflow:loss = 41.47454, step = 1301 (1.261 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.2573\n",
      "INFO:tensorflow:loss = 40.094524, step = 1401 (2.118 sec)\n",
      "INFO:tensorflow:global_step/sec: 60.6971\n",
      "INFO:tensorflow:loss = 40.830116, step = 1501 (1.645 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.5882\n",
      "INFO:tensorflow:loss = 35.168465, step = 1601 (2.145 sec)\n",
      "INFO:tensorflow:global_step/sec: 65.5007\n",
      "INFO:tensorflow:loss = 42.872295, step = 1701 (1.533 sec)\n",
      "INFO:tensorflow:global_step/sec: 64.3088\n",
      "INFO:tensorflow:loss = 34.627037, step = 1801 (1.549 sec)\n",
      "INFO:tensorflow:global_step/sec: 55.6427\n",
      "INFO:tensorflow:loss = 38.30638, step = 1901 (1.800 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into C:\\Users\\Florian\\AppData\\Local\\Temp\\tmpb7bbyi_y\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 33.46254.\n",
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\Florian\\AppData\\Local\\Temp\\tmpp5msvx0o\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Users\\\\Florian\\\\AppData\\\\Local\\\\Temp\\\\tmpp5msvx0o', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000017502CCD240>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From c:\\users\\florian\\anaconda3\\envs\\aix360\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:Entity <bound method _DNNModel.call of <tensorflow_estimator.python.estimator.canned.dnn._DNNModel object at 0x0000017506BA1EF0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method _DNNModel.call of <tensorflow_estimator.python.estimator.canned.dnn._DNNModel object at 0x0000017506BA1EF0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method _DNNModel.call of <tensorflow_estimator.python.estimator.canned.dnn._DNNModel object at 0x0000017506BA1EF0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method _DNNModel.call of <tensorflow_estimator.python.estimator.canned.dnn._DNNModel object at 0x0000017506BA1EF0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method DenseFeatures.call of <tensorflow.python.feature_column.feature_column_v2.DenseFeatures object at 0x0000017506B82860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method DenseFeatures.call of <tensorflow.python.feature_column.feature_column_v2.DenseFeatures object at 0x0000017506B82860>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method DenseFeatures.call of <tensorflow.python.feature_column.feature_column_v2.DenseFeatures object at 0x0000017506B82860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method DenseFeatures.call of <tensorflow.python.feature_column.feature_column_v2.DenseFeatures object at 0x0000017506B82860>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "DEBUG:tensorflow:Transforming feature_column NumericColumn(key='age', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None).\n",
      "DEBUG:tensorflow:Transforming feature_column IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='age_cat', vocabulary_list=('Greater than 45', '25 - 45', 'Less than 25'), dtype=tf.string, default_value=-1, num_oov_buckets=0)).\n",
      "DEBUG:tensorflow:Transforming feature_column VocabularyListCategoricalColumn(key='age_cat', vocabulary_list=('Greater than 45', '25 - 45', 'Less than 25'), dtype=tf.string, default_value=-1, num_oov_buckets=0).\n",
      "DEBUG:tensorflow:Transforming feature_column IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='c_charge_degree', vocabulary_list=('F', 'M'), dtype=tf.string, default_value=-1, num_oov_buckets=0)).\n",
      "DEBUG:tensorflow:Transforming feature_column VocabularyListCategoricalColumn(key='c_charge_degree', vocabulary_list=('F', 'M'), dtype=tf.string, default_value=-1, num_oov_buckets=0).\n",
      "DEBUG:tensorflow:Transforming feature_column NumericColumn(key='days_b_screening_arrest', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None).\n",
      "DEBUG:tensorflow:Transforming feature_column NumericColumn(key='decile_score', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:tensorflow:Transforming feature_column NumericColumn(key='length_of_stay', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None).\n",
      "DEBUG:tensorflow:Transforming feature_column NumericColumn(key='priors_count', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None).\n",
      "DEBUG:tensorflow:Transforming feature_column IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='race', vocabulary_list=('Other', 'African-American', 'Caucasian', 'Hispanic', 'Asian', 'Native American'), dtype=tf.string, default_value=-1, num_oov_buckets=0)).\n",
      "DEBUG:tensorflow:Transforming feature_column VocabularyListCategoricalColumn(key='race', vocabulary_list=('Other', 'African-American', 'Caucasian', 'Hispanic', 'Asian', 'Native American'), dtype=tf.string, default_value=-1, num_oov_buckets=0).\n",
      "DEBUG:tensorflow:Transforming feature_column IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='score_text', vocabulary_list=('Low', 'Medium', 'High'), dtype=tf.string, default_value=-1, num_oov_buckets=0)).\n",
      "DEBUG:tensorflow:Transforming feature_column VocabularyListCategoricalColumn(key='score_text', vocabulary_list=('Low', 'Medium', 'High'), dtype=tf.string, default_value=-1, num_oov_buckets=0).\n",
      "DEBUG:tensorflow:Transforming feature_column IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='sex', vocabulary_list=('Male', 'Female'), dtype=tf.string, default_value=-1, num_oov_buckets=0)).\n",
      "DEBUG:tensorflow:Transforming feature_column VocabularyListCategoricalColumn(key='sex', vocabulary_list=('Male', 'Female'), dtype=tf.string, default_value=-1, num_oov_buckets=0).\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000017506BC2940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000017506BC2940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000017506BC2940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000017506BC2940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000175067D3198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000175067D3198>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000175067D3198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000175067D3198>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000017506C14550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000017506C14550>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000017506C14550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000017506C14550>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000017506C14780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000017506C14780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000017506C14780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000017506C14780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:From c:\\users\\florian\\anaconda3\\envs\\aix360\\lib\\site-packages\\tensorflow\\python\\training\\adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into C:\\Users\\Florian\\AppData\\Local\\Temp\\tmpp5msvx0o\\model.ckpt.\n",
      "INFO:tensorflow:loss = 50.071518, step = 1\n",
      "INFO:tensorflow:global_step/sec: 67.2948\n",
      "INFO:tensorflow:loss = 41.063866, step = 101 (1.489 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.7458\n",
      "INFO:tensorflow:loss = 35.117306, step = 201 (1.307 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.0417\n",
      "INFO:tensorflow:loss = 40.13932, step = 301 (1.299 sec)\n",
      "INFO:tensorflow:global_step/sec: 65.9196\n",
      "INFO:tensorflow:loss = 38.879406, step = 401 (1.519 sec)\n",
      "INFO:tensorflow:global_step/sec: 66.2687\n",
      "INFO:tensorflow:loss = 40.40056, step = 501 (1.503 sec)\n",
      "INFO:tensorflow:global_step/sec: 63.4521\n",
      "INFO:tensorflow:loss = 30.572464, step = 601 (1.575 sec)\n",
      "INFO:tensorflow:global_step/sec: 110.497\n",
      "INFO:tensorflow:loss = 35.98202, step = 701 (0.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 122.1\n",
      "INFO:tensorflow:loss = 35.095932, step = 801 (0.819 sec)\n",
      "INFO:tensorflow:global_step/sec: 117.785\n",
      "INFO:tensorflow:loss = 35.661278, step = 901 (0.849 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.1839\n",
      "INFO:tensorflow:loss = 39.95561, step = 1001 (1.147 sec)\n",
      "INFO:tensorflow:global_step/sec: 101.317\n",
      "INFO:tensorflow:loss = 32.763466, step = 1101 (0.986 sec)\n",
      "INFO:tensorflow:global_step/sec: 120.773\n",
      "INFO:tensorflow:loss = 34.128212, step = 1201 (0.829 sec)\n",
      "INFO:tensorflow:global_step/sec: 122.1\n",
      "INFO:tensorflow:loss = 40.23203, step = 1301 (0.819 sec)\n",
      "INFO:tensorflow:global_step/sec: 125\n",
      "INFO:tensorflow:loss = 34.59715, step = 1401 (0.800 sec)\n",
      "INFO:tensorflow:global_step/sec: 124.844\n",
      "INFO:tensorflow:loss = 40.612656, step = 1501 (0.801 sec)\n",
      "INFO:tensorflow:global_step/sec: 124.688\n",
      "INFO:tensorflow:loss = 35.56938, step = 1601 (0.802 sec)\n",
      "INFO:tensorflow:global_step/sec: 111.856\n",
      "INFO:tensorflow:loss = 38.588615, step = 1701 (0.894 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.9656\n",
      "INFO:tensorflow:loss = 31.673319, step = 1801 (1.450 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 39.541527, step = 1901 (0.971 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into C:\\Users\\Florian\\AppData\\Local\\Temp\\tmpp5msvx0o\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 38.162025.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.canned.dnn.DNNClassifier at 0x175027df940>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title Convert dataset to tf.Example protos {display-mode: \"form\"}\n",
    "examples = df_to_examples(x_train)\n",
    "\n",
    "#@title Create and train the DNN classifier {display-mode: \"form\"}\n",
    "tf.logging.set_verbosity(tf.logging.DEBUG)\n",
    "num_steps = 2000  #@param {type: \"number\"}\n",
    "num_steps_2 = 2000  #@param {type: \"number\"}\n",
    "\n",
    "feature_spec = create_feature_spec(x_train, features_and_labels)\n",
    "\n",
    "train_inpf = functools.partial(tfexamples_input_fn, examples, feature_spec, label_column)\n",
    "\n",
    "# define and train linear model\n",
    "lin_classifier = tf.estimator.LinearClassifier(\n",
    "    feature_columns=create_feature_columns(input_features, feature_spec))\n",
    "lin_classifier.train(train_inpf, steps=num_steps)\n",
    "\n",
    "# define and train non-linear model\n",
    "nonlin_classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=create_feature_columns(input_features, feature_spec),\n",
    "    hidden_units=[128, 64, 32])\n",
    "nonlin_classifier.train(train_inpf, steps=num_steps_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_height_in_px = 600  #@param {type: \"number\"}\n",
    "\n",
    "test_examples = df_to_examples(x_test)\n",
    "\n",
    "# Setup the tool with the test examples and the trained classifier\n",
    "config_builder = WitConfigBuilder(test_examples).set_estimator_and_feature_spec(\n",
    "    lin_classifier, feature_spec).set_compare_estimator_and_feature_spec(\n",
    "    nonlin_classifier, feature_spec).set_label_vocab(['not_two_year_recid', 'two_year_recid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0672af09230b41cd8d2f0cf43b6c2971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "WitWidget(config={'model_type': 'classification', 'label_vocab': ['not_two_year_recid', 'two_year_recid'], 'ar…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method LinearModel.call of <tensorflow.python.feature_column.feature_column_v2.LinearModel object at 0x0000017502900FD0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LinearModel.call of <tensorflow.python.feature_column.feature_column_v2.LinearModel object at 0x0000017502900FD0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method LinearModel.call of <tensorflow.python.feature_column.feature_column_v2.LinearModel object at 0x0000017502900FD0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LinearModel.call of <tensorflow.python.feature_column.feature_column_v2.LinearModel object at 0x0000017502900FD0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method LinearModel.call of <tensorflow.python.feature_column.feature_column_v2.LinearModel object at 0x0000017502900FD0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LinearModel.call of <tensorflow.python.feature_column.feature_column_v2.LinearModel object at 0x0000017502900FD0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method _LinearModelLayer.call of <tensorflow.python.feature_column.feature_column_v2._LinearModelLayer object at 0x00000175029009E8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method _LinearModelLayer.call of <tensorflow.python.feature_column.feature_column_v2._LinearModelLayer object at 0x00000175029009E8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method _LinearModelLayer.call of <tensorflow.python.feature_column.feature_column_v2._LinearModelLayer object at 0x00000175029009E8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method _LinearModelLayer.call of <tensorflow.python.feature_column.feature_column_v2._LinearModelLayer object at 0x00000175029009E8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method _LinearModelLayer.call of <tensorflow.python.feature_column.feature_column_v2._LinearModelLayer object at 0x00000175029009E8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method _LinearModelLayer.call of <tensorflow.python.feature_column.feature_column_v2._LinearModelLayer object at 0x00000175029009E8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:From c:\\users\\florian\\anaconda3\\envs\\aix360\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\florian\\anaconda3\\envs\\aix360\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method _DNNModel.call of <tensorflow_estimator.python.estimator.canned.dnn._DNNModel object at 0x0000017505190860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method _DNNModel.call of <tensorflow_estimator.python.estimator.canned.dnn._DNNModel object at 0x0000017505190860>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method _DNNModel.call of <tensorflow_estimator.python.estimator.canned.dnn._DNNModel object at 0x0000017505190860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method _DNNModel.call of <tensorflow_estimator.python.estimator.canned.dnn._DNNModel object at 0x0000017505190860>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method _DNNModel.call of <tensorflow_estimator.python.estimator.canned.dnn._DNNModel object at 0x0000017505190860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method _DNNModel.call of <tensorflow_estimator.python.estimator.canned.dnn._DNNModel object at 0x0000017505190860>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method DenseFeatures.call of <tensorflow.python.feature_column.feature_column_v2.DenseFeatures object at 0x00000175051903C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method DenseFeatures.call of <tensorflow.python.feature_column.feature_column_v2.DenseFeatures object at 0x00000175051903C8>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method DenseFeatures.call of <tensorflow.python.feature_column.feature_column_v2.DenseFeatures object at 0x00000175051903C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method DenseFeatures.call of <tensorflow.python.feature_column.feature_column_v2.DenseFeatures object at 0x00000175051903C8>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method DenseFeatures.call of <tensorflow.python.feature_column.feature_column_v2.DenseFeatures object at 0x00000175051903C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method DenseFeatures.call of <tensorflow.python.feature_column.feature_column_v2.DenseFeatures object at 0x00000175051903C8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000175054D4F28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000175054D4F28>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000175054D4F28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000175054D4F28>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000175054D4F28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000175054D4F28>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000175051906A0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000175051906A0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000175051906A0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000175051906A0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000175051906A0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000175051906A0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000017507E30048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000017507E30048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000017507E30048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000017507E30048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000017507E30048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000017507E30048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000017507E30390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000017507E30390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000017507E30390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000017507E30390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000017507E30390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000017507E30390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    }
   ],
   "source": [
    "# Calling What-If Tool via the witwidget package\n",
    "# Click Cell -> Current Outputs -> Toggle Scrolling if WIT is not shown as tool_height_in_px\n",
    "WitWidget(config_builder, height=tool_height_in_px)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples\n",
    "\n",
    "#### Visualize distribution\n",
    "If we use a binning on the feature 'race' we can visualize the skewed distribution in our dataset. By far the most people are African-Americans and they also have in contrast to the other races more positive predictions. About half of the samples with the feature {race = African-American} get predicted as true in contrast to the other groups, where around one third gets predicted as true.\n",
    "\n",
    "<img src=\"./example_distribution.png\" alt=\"drawing\" width=\"800\"/>\n",
    "\n",
    "#### Search Counterfactual\n",
    "There is also the functionality to search for counterfactuals of a specific sample. Just click on one sample and choose \"Nearest counterfactual\". The counterfactual is the nearest (L1 or L2 distance) sample of the group that was contrary predicted from the model.\n",
    "\n",
    "<img src=\"./example_counterfactual.png\" alt=\"drawing\" width=\"800\"/>\n",
    "\n",
    "#### Use Fairness Algortihm\n",
    "In the 'Performance and Fairness' tab are a few standard algorithms for bias mitigation. In the picture below we set the ground truth label to 'two_year_recid' and sliced by the feature 'race'. We chose the 'Equal Opportunity' algorithm that signifies that there is a bias for positive predictions in the African-American group. [Here](https://pair-code.github.io/what-if-tool/ai-fairness.html) you can find more details to those five fairness algorithms.\n",
    "\n",
    "<img src=\"./example_fairness.png\" alt=\"drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.A. AI Explainability 360\n",
    "\n",
    "AI Explainability 360 is an open source toolkit that gives access to various algorithms to comprehend the predictions of your models. There is a growing number of algorithms state of the art based on recent published papers. You can find a documentation of the toolkit [here](https://aix360.readthedocs.io/en/latest/).\n",
    "\n",
    "In this notebook we installed the official pip package [aix360](https://pypi.org/project/aix360/) and tested two algorithms on simple models trained on our COMPAS dataset. First off we used a given linear model of the package based on the implementation  of the logistic regression class of [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). Secondly, we trained our dataset on a two layer neural net implemented in Pytorch and evaluated the results with the [Protodash Explainer](https://aix360.readthedocs.io/en/latest/die.html#aix360.algorithms.protodash.PDASH.ProtodashExplainer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to use existing split\n",
    "yTrain_raw = x_train.pop('two_year_recid')\n",
    "dfTrain_raw = x_train\n",
    "yTest_raw = x_test.pop('two_year_recid')\n",
    "dfTest_raw = x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\florian\\anaconda3\\envs\\aix360\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "c:\\users\\florian\\anaconda3\\envs\\aix360\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "c:\\users\\florian\\anaconda3\\envs\\aix360\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# Convert few columns so that Feature Binarizer has no problems handeling these\n",
    "dfTrain_raw['age'] = dfTrain_raw['age'].astype(np.int)\n",
    "dfTrain_raw['priors_count'] = dfTrain_raw['priors_count'].astype(np.int)\n",
    "dfTrain_raw['decile_score'] = dfTrain_raw['decile_score'].astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Rule Regression\n",
    "\n",
    "This is a given model from the aix360 package that is based on the logistic regression class from [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). We first transform our feature to binarize features with the [FeatureBinarizer](https://github.com/IBM/AIX360/blob/master/aix360/algorithms/rbm/features.py) class from aix360. This will create additional features that are either True or False (rules)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping column 'length_of_stay': data type cannot be handled\n",
      "Skipping column 'length_of_stay': data type cannot be handled\n",
      "Skipping column 'length_of_stay': data type cannot be handled\n",
      "(4791, 82)\n"
     ]
    }
   ],
   "source": [
    "# Binarize data and also return standardized ordinal features\n",
    "from aix360.algorithms.rbm import FeatureBinarizer\n",
    "fb = FeatureBinarizer(colCateg=[1,2,3,4,5], negations=True, returnOrd=True)\n",
    "dfTrain, dfTrainStd = fb.fit_transform(dfTrain_raw)\n",
    "dfTest, dfTestStd = fb.transform(dfTest_raw)\n",
    "print(dfTrain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After preprocessing our data we create the [LogisticRuleRegression (LRR)](https://aix360.readthedocs.io/en/latest/dise.html#aix360.algorithms.rbm.logistic_regression.LogisticRuleRegression) model and train it on our dataset. The LLR model has a few explain functions already implemented. Hence we can directly call explain() on our model and get the most important features listed. Additional we can visualize the impact of a feature value for each feature.\n",
    "\n",
    "In the plotted diagrams below we can evaluate that males are more likely to be predicted as two-year recidivist and also that the impact of prior_count grows linear after prior_count = 2. This indicates that we have probably a shifted distribution in prior counts as a detainee with less then two prior counts is more likely to be predicted as a recidivist than detainess with prior counts between two and ten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.6892089334168232\n",
      "Test accuracy: 0.7186978297161937\n",
      "Probability of Y=1 is predicted as logistic(z) = 1 / (1 + exp(-z))\n",
      "where z is a linear combination of the following rules/numerical features:\n"
     ]
    }
   ],
   "source": [
    "# Instantiate LRR with good complexity penalties and numerical features\n",
    "from aix360.algorithms.rbm import LogisticRuleRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "lrr = LogisticRuleRegression(lambda0=0.005, lambda1=0.001, useOrd=True)\n",
    "\n",
    "# Train, print, and evaluate model\n",
    "lrr.fit(dfTrain, yTrain_raw, dfTrainStd)\n",
    "print('Training accuracy:', accuracy_score(yTrain_raw, lrr.predict(dfTrain, dfTrainStd)))\n",
    "print('Test accuracy:', accuracy_score(yTest_raw, lrr.predict(dfTest, dfTestStd)))\n",
    "print('Probability of Y=1 is predicted as logistic(z) = 1 / (1 + exp(-z))')\n",
    "print('where z is a linear combination of the following rules/numerical features:')\n",
    "dfExplain = lrr.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rule/numerical feature</th>\n",
       "      <th>coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(intercept)</td>\n",
       "      <td>0.784971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>priors_count &lt;= 2.00 AND days_b_screening_arre...</td>\n",
       "      <td>-0.939935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>age &gt; 22.00 AND days_b_screening_arrest &lt;= 0.00</td>\n",
       "      <td>-0.729502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>priors_count</td>\n",
       "      <td>0.722026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>age</td>\n",
       "      <td>-0.538052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>priors_count &lt;= 2.00</td>\n",
       "      <td>0.488433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sex</td>\n",
       "      <td>0.40147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>score_text != High AND days_b_screening_arrest...</td>\n",
       "      <td>-0.395099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>priors_count &lt;= 1.00</td>\n",
       "      <td>-0.394286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>score_text == Low AND days_b_screening_arrest ...</td>\n",
       "      <td>-0.349061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>priors_count &lt;= 0.00</td>\n",
       "      <td>-0.346311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>decile_score</td>\n",
       "      <td>0.300797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>age &lt;= 31.00</td>\n",
       "      <td>0.237248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>days_b_screening_arrest</td>\n",
       "      <td>0.216947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>c_charge_degree   AND days_b_screening_arrest ...</td>\n",
       "      <td>-0.155727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>days_b_screening_arrest &lt;= 0.00 AND decile_sco...</td>\n",
       "      <td>-0.139273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               rule/numerical feature coefficient\n",
       "0                                         (intercept)    0.784971\n",
       "1   priors_count <= 2.00 AND days_b_screening_arre...   -0.939935\n",
       "2     age > 22.00 AND days_b_screening_arrest <= 0.00   -0.729502\n",
       "3                                        priors_count    0.722026\n",
       "4                                                 age   -0.538052\n",
       "5                                priors_count <= 2.00    0.488433\n",
       "6                                               sex       0.40147\n",
       "7   score_text != High AND days_b_screening_arrest...   -0.395099\n",
       "8                                priors_count <= 1.00   -0.394286\n",
       "9   score_text == Low AND days_b_screening_arrest ...   -0.349061\n",
       "10                               priors_count <= 0.00   -0.346311\n",
       "11                                       decile_score    0.300797\n",
       "12                                       age <= 31.00    0.237248\n",
       "13                            days_b_screening_arrest    0.216947\n",
       "14  c_charge_degree   AND days_b_screening_arrest ...   -0.155727\n",
       "15  days_b_screening_arrest <= 0.00 AND decile_sco...   -0.139273"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# returns a list of features with the most impact for a positive or negative prediction\n",
    "lrr.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'sex': <Figure size 432x288 with 1 Axes>}, sex    0.024844\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrr.visualize(dataset, fb, ['sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'priors_count': <Figure size 432x288 with 1 Axes>}, priors_count    0.153856\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrr.visualize(dataset, fb, ['priors_count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Net\n",
    "\n",
    "For the DNN example we will use the [Protodash](https://aix360.readthedocs.io/en/latest/die.html#aix360.algorithms.protodash.PDASH.ProtodashExplainer) algorithm of the aix360 package to evaluate our own defined model. The Protodash algorithm can be used to explain predictions of models or a dataset. In our case we implemented a simple neural net in Pytorch and trained it on our COMPAS dataset. Lastly we made predictions on our test dataset and evaluated the results with the Protodash algorithm.\n",
    "\n",
    "This algorithm is based on a recently published paper from [Karthik S. Gurumoorthy](https://arxiv.org/pdf/1707.01212.pdf) with involvment of the IBM research team. It basically searches a dataset for n prototypes given your example data by minimizing the maximum mean discrepancy. We use it to find the most similiar n prototypes given one example we have chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms categorical columns in numerical\n",
    "def encode(X, cat_cols):\n",
    "    enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "    enc.fit(X)\n",
    "    df = pd.DataFrame(enc.transform(X), columns= enc.get_feature_names(cat_cols))\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode categorical columns with OneHotEncoder from scikit\n",
    "\n",
    "categorical_columns = ['c_charge_degree', 'race', 'age_cat', 'score_text', 'sex']\n",
    "\n",
    "# extract categorical columns in a new dataframe\n",
    "train_x_cat = dfTrain_raw.loc[:, categorical_columns]\n",
    "test_x_cat = dfTest_raw.loc[:, categorical_columns]\n",
    "\n",
    "# remove categorical columns\n",
    "train_x = dfTrain_raw.drop(columns=categorical_columns)\n",
    "test_x = dfTest_raw.drop(columns=categorical_columns)\n",
    "\n",
    "# reset index for concat\n",
    "train_x.reset_index(drop=True, inplace=True)\n",
    "test_x.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# concat again with encoded categorical columns\n",
    "train_x = pd.concat([train_x, encode(train_x_cat, categorical_columns)], axis=1)\n",
    "test_x = pd.concat([test_x, encode(test_x_cat, categorical_columns)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper method to organize features and labels in a tuple for pytorch dataloader\n",
    "def merge_features_labels(x, y):\n",
    "    merge = []\n",
    "    for i in range(x.shape[0]):\n",
    "        tup = (torch.Tensor(x[i]),torch.Tensor(y[i]))\n",
    "        merge.append(tup)\n",
    "    return merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1,    10] loss: 0.000, Accuracy: 0.582\n",
      "Epoch [2,    10] loss: 0.000, Accuracy: 0.673\n",
      "Epoch [3,    10] loss: 0.000, Accuracy: 0.600\n",
      "Epoch [4,    10] loss: 0.000, Accuracy: 0.636\n",
      "Epoch [5,    10] loss: 0.000, Accuracy: 0.818\n",
      "Epoch [6,    10] loss: 0.000, Accuracy: 0.655\n",
      "Epoch [7,    10] loss: 0.000, Accuracy: 0.673\n",
      "Epoch [8,    10] loss: 0.000, Accuracy: 0.764\n",
      "Epoch [9,    10] loss: 0.000, Accuracy: 0.618\n",
      "Epoch [10,    10] loss: 0.000, Accuracy: 0.691\n"
     ]
    }
   ],
   "source": [
    "input_dim = train_x.shape[1]\n",
    "\n",
    "xTrain = torch.Tensor(np.array(train_x))\n",
    "yTrain = torch.Tensor(np.array(yTrain_raw.values.reshape(yTrain_raw.shape[0],1)))\n",
    "\n",
    "xTest = torch.Tensor(np.array(test_x))\n",
    "yTest = torch.Tensor(np.array(yTest_raw.values.reshape(yTest_raw.shape[0],1)))\n",
    "\n",
    "# organize data in a tuples\n",
    "train_dat = merge_features_labels(np.array(train_x), np.array(yTrain_raw.values.reshape(yTrain_raw.shape[0],1)))\n",
    "\n",
    "# define model\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(input_dim, 5),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(5, 1),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "# model pipeline\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    train_dat,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# train n epochs our model\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        \n",
    "        inputs, labels = data\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        # print statistics\n",
    "    running_loss += loss.item()\n",
    "    running_loss = 0.0\n",
    "    y_hat_test = model(inputs)\n",
    "    y_hat_test_class = np.where(y_hat_test.detach().numpy()<0.5, 0, 1)\n",
    "    test_accuracy = np.sum(labels.detach().numpy().reshape(-1,1)==y_hat_test_class) / len(labels.detach().numpy())\n",
    "    print('Epoch [%d, %5d] loss: %.3f, Accuracy: %.3f' % (epoch + 1, epochs, running_loss / 2000, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we trained our model we can now evaluate it with the [Protodash](https://aix360.readthedocs.io/en/latest/die.html#aix360.algorithms.protodash.PDASH.ProtodashExplainer) algorithm. We first store all predictions of our training data in 'p_train' and filter only the good examples (where a detainee is in two years recid). Then we pick a random sample from our test dataset and print it.\n",
    "\n",
    "After this we run the ProtodashExplainer() with our chosen random sample, all good examples of our training dataset and a number of prototypes that we want the algorithm to return. Protodash searches for the most similar  samples given our chosen random sample. Lastly, we print the most similar prototypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaluation\n",
    "\n",
    "# predict a datapoint\n",
    "p_train = model(xTrain) # Use trained neural network to predict train points\n",
    "p_train = p_train.reshape((p_train.shape[0],1))\n",
    "\n",
    "z_train = np.hstack((xTrain.detach(), p_train.detach()))\n",
    "\n",
    "mask = (z_train[:,-1]>0.5)\n",
    "\n",
    "# array with the good predicted samples\n",
    "z_train_good = z_train[mask]\n",
    "# array with real values of the good predicted samples\n",
    "y_z_train_good = yTrain[mask].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen Sample: 20\n",
      "Prediction made by the model: recid\n",
      "Prediction probabilities: tensor([[0.9999]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx = 20\n",
    "\n",
    "class_names = ['not_recid', 'recid']\n",
    "\n",
    "X = xTest[idx].reshape((1,) + xTest[idx].shape)\n",
    "\n",
    "print(\"Chosen Sample:\", idx)\n",
    "# print(\"Prediction made by the model:\", class_names[np.argmax(model(X).detach())]) # 0.5\n",
    "print(\"Prediction made by the model:\", class_names[1 if model(X).detach() > 0.5 else 0]) # 0.5\n",
    "print(\"Prediction probabilities:\", model(X).detach())\n",
    "print(\"\")\n",
    "\n",
    "# attach the prediction made by the model to X\n",
    "X = np.hstack((X, model(X).detach().reshape((1,1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define columns to create dataframes\n",
    "cols = train_x.columns\n",
    "df_tmp = pd.Index(['two_year_recid']) \n",
    "cols = cols.append(df_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>priors_count</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>days_b_screening_arrest</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decile_score</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>length_of_stay</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c_charge_degree_F</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c_charge_degree_M</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race_African-American</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race_Asian</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race_Caucasian</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race_Hispanic</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race_Native American</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race_Other</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_cat_25 - 45</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_cat_Greater than 45</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_cat_Less than 25</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_text_High</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_text_Low</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_text_Medium</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sex_Female</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sex_Male</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>two_year_recid</th>\n",
       "      <td>recid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gt_two_year_recid</th>\n",
       "      <td>recid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weight</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             0\n",
       "age                         33\n",
       "priors_count                14\n",
       "days_b_screening_arrest     -1\n",
       "decile_score                10\n",
       "length_of_stay               1\n",
       "c_charge_degree_F            1\n",
       "c_charge_degree_M            0\n",
       "race_African-American        1\n",
       "race_Asian                   0\n",
       "race_Caucasian               0\n",
       "race_Hispanic                0\n",
       "race_Native American         0\n",
       "race_Other                   0\n",
       "age_cat_25 - 45              1\n",
       "age_cat_Greater than 45      0\n",
       "age_cat_Less than 25         0\n",
       "score_text_High              1\n",
       "score_text_Low               0\n",
       "score_text_Medium            0\n",
       "sex_Female                   0\n",
       "sex_Male                     1\n",
       "two_year_recid           recid\n",
       "gt_two_year_recid        recid\n",
       "Weight                       1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show chosen example\n",
    "dfx = pd.DataFrame.from_records(X.astype('double')) # Create dataframe with original feature values\n",
    "dfx.iloc[0,21] = class_names[1 if dfx.iloc[0,21] > 0.5 else 0]\n",
    "dfx.columns = cols\n",
    "dfx['gt_two_year_recid'] = class_names[1 if yTest[idx] > 0.5 else 0]\n",
    "dfx['Weight'] = 1.0\n",
    "dfx.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  0.0000e+00 -2.0000e+04  4e+00  1e+00  1e+00\n",
      " 1:  1.0542e+03 -1.3334e+07  2e+03  1e+00  1e+00\n",
      " 2:  1.1446e+03 -1.4897e+07  2e+03  1e+00  1e+00\n",
      " 3:  1.6845e+03 -2.2851e+07  3e+03  1e+00  1e+00\n",
      " 4:  2.9933e+03 -3.6733e+07  6e+03  1e+00  1e+00\n",
      " 5:  3.0081e+03 -3.6886e+07  6e+03  1e+00  1e+00\n",
      " 6:  3.2222e+03 -5.7534e+07  1e+04  1e+00  1e+00\n",
      " 7: -9.2386e+01 -9.0073e+07  2e+04  1e+00  1e+00\n",
      " 8:  1.0664e+02 -2.2897e+08  5e+04  1e+00  1e+00\n",
      " 9:  1.7970e+02 -6.8443e+08  1e+05  1e+00  1e+00\n",
      "10:  3.6337e+02 -4.1474e+09  9e+05  1e+00  1e+00\n",
      "11:  8.0863e+02 -1.4243e+11  3e+07  1e+00  1e+00\n",
      "12:  8.0769e+02 -3.3021e+14  1e+11  1e+00  1e+00\n",
      "13:  2.2315e+10 -3.4485e+21  3e+21  4e-13  2e+01\n",
      "14:  2.2315e+10 -3.4485e+19  3e+19  4e-15  2e-01\n",
      "15:  2.2315e+10 -3.4485e+17  3e+17  1e-16  2e-03\n",
      "16:  2.2315e+10 -3.4486e+15  3e+15  6e-17  2e-05\n",
      "17:  2.2313e+10 -3.4527e+13  3e+13  2e-16  2e-07\n",
      "18:  2.2105e+10 -3.8633e+11  4e+11  2e-16  1e-07\n",
      "19:  1.2013e+10 -2.5210e+10  4e+10  1e-16  4e-07\n",
      "20:  1.8022e+09 -2.3901e+09  4e+09  1e-16  2e-12\n",
      "21:  2.6183e+08 -2.9400e+08  6e+08  2e-16  3e-13\n",
      "22:  3.7566e+07 -4.2110e+07  8e+07  3e-17  2e-13\n",
      "23:  5.3570e+06 -6.0195e+06  1e+07  1e-16  5e-14\n",
      "24:  7.5550e+05 -8.7329e+05  2e+06  2e-16  1e-14\n",
      "25:  1.0337e+05 -1.2984e+05  2e+05  3e-16  1e-14\n",
      "26:  1.2779e+04 -2.0540e+04  3e+04  1e-16  2e-15\n",
      "27:  8.7685e+02 -3.8075e+03  5e+03  2e-16  4e-16\n",
      "28: -4.5717e+02 -1.0403e+03  6e+02  2e-16  1e-16\n",
      "29: -6.6545e+02 -8.7933e+02  2e+02  2e-16  2e-16\n",
      "30: -6.8989e+02 -7.0810e+02  2e+01  1e-16  1e-16\n",
      "31: -6.9545e+02 -6.9766e+02  2e+00  1e-16  7e-17\n",
      "32: -6.9613e+02 -6.9643e+02  3e-01  3e-16  1e-16\n",
      "33: -6.9619e+02 -6.9622e+02  3e-02  3e-16  2e-16\n",
      "34: -6.9619e+02 -6.9619e+02  5e-04  3e-16  1e-16\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  0.0000e+00 -3.0000e+04  6e+00  1e+00  1e+00\n",
      " 1:  4.1878e+02 -8.0739e+06  1e+03  1e+00  1e+00\n",
      " 2:  3.4972e+03 -5.3115e+07  9e+03  1e+00  1e+00\n",
      " 3:  3.8887e+03 -5.8920e+07  1e+04  1e+00  1e+00\n",
      " 4:  1.5423e+03 -7.5818e+07  1e+04  1e+00  1e+00\n",
      " 5:  4.7727e+03 -1.7028e+08  3e+04  1e+00  1e+00\n",
      " 6:  1.5774e+03 -4.1433e+09  8e+05  1e+00  1e+00\n",
      " 7:  2.9018e+03 -2.3938e+11  5e+07  1e+00  1e+00\n",
      " 8:  1.9700e+10 -5.6266e+18  6e+18  5e-13  5e-05\n",
      " 9:  1.9700e+10 -5.6266e+16  6e+16  4e-15  2e-05\n",
      "10:  1.9700e+10 -5.6299e+14  6e+14  2e-16  5e-07\n",
      "11:  1.9691e+10 -5.9580e+12  6e+12  2e-16  8e-09\n",
      "12:  1.8828e+10 -3.7107e+11  4e+11  3e-16  6e-10\n",
      "13:  1.0034e+09 -2.0337e+11  2e+11  2e-16  2e-09\n",
      "14:  5.5779e+08 -3.3385e+09  4e+09  1e-16  3e-11\n",
      "15:  8.8537e+07 -1.2677e+08  2e+08  1e-16  8e-13\n",
      "16:  1.2612e+07 -1.3805e+07  3e+07  1e-16  6e-14\n",
      "17:  1.7874e+06 -2.0464e+06  4e+06  1e-16  3e-14\n",
      "18:  2.4772e+05 -3.0053e+05  5e+05  1e-16  1e-14\n",
      "19:  3.2089e+04 -4.6364e+04  8e+04  2e-16  2e-15\n",
      "20:  3.0834e+03 -8.0511e+03  1e+04  3e-16  1e-15\n",
      "21: -3.7938e+02 -1.8717e+03  1e+03  2e-16  5e-16\n",
      "22: -6.6708e+02 -8.0150e+02  1e+02  2e-16  1e-16\n",
      "23: -6.9023e+02 -7.0572e+02  2e+01  1e-16  6e-17\n",
      "24: -6.9564e+02 -6.9787e+02  2e+00  2e-16  1e-16\n",
      "25: -6.9629e+02 -6.9657e+02  3e-01  1e-16  6e-17\n",
      "26: -6.9636e+02 -6.9639e+02  3e-02  2e-16  1e-16\n",
      "27: -6.9636e+02 -6.9636e+02  1e-03  2e-16  1e-16\n",
      "28: -6.9636e+02 -6.9636e+02  1e-05  3e-16  1e-16\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  0.0000e+00 -4.0000e+04  8e+00  1e+00  1e+00\n",
      " 1:  3.2115e+02 -8.5591e+06  1e+03  1e+00  1e+00\n",
      " 2:  1.0446e+03 -3.0281e+07  4e+03  1e+00  1e+00\n",
      " 3:  4.0462e+03 -8.4766e+07  1e+04  1e+00  1e+00\n",
      " 4:  4.1726e+03 -8.6758e+07  1e+04  1e+00  1e+00\n",
      " 5:  6.5220e+03 -1.2121e+08  2e+04  1e+00  1e+00\n",
      " 6:  5.5933e+03 -1.2864e+08  2e+04  1e+00  1e+00\n",
      " 7:  6.0106e+03 -1.4648e+08  3e+04  1e+00  1e+00\n",
      " 8:  7.5834e+03 -4.1485e+08  8e+04  1e+00  1e+00\n",
      " 9:  5.4641e+03 -7.7256e+10  2e+07  1e+00  1e+00\n",
      "10:  7.6983e+03 -1.4337e+14  4e+10  1e+00  1e+00\n",
      "11:  9.5819e+09 -1.6780e+21  2e+21  3e-13  6e-03\n",
      "12:  9.5819e+09 -1.6780e+19  2e+19  3e-15  7e-03\n",
      "13:  9.5819e+09 -1.6780e+17  2e+17  2e-16  6e-05\n",
      "14:  9.5818e+09 -1.6781e+15  2e+15  2e-16  4e-07\n",
      "15:  9.5785e+09 -1.6842e+13  2e+13  2e-16  5e-09\n",
      "16:  9.2604e+09 -2.2815e+11  2e+11  1e-16  1e-10\n",
      "17:  2.8487e+09 -1.4129e+10  2e+10  2e-16  9e-12\n",
      "18:  4.3120e+08 -5.6964e+08  1e+09  1e-16  2e-13\n",
      "19:  6.1789e+07 -6.8747e+07  1e+08  3e-16  2e-13\n",
      "20:  8.8226e+06 -9.9240e+06  2e+07  2e-16  6e-14\n",
      "21:  1.2468e+06 -1.4340e+06  3e+06  2e-16  1e-14\n",
      "22:  1.7162e+05 -2.1225e+05  4e+05  2e-16  1e-14\n",
      "23:  2.1696e+04 -3.3190e+04  5e+04  7e-17  2e-15\n",
      "24:  1.7875e+03 -5.9703e+03  8e+03  3e-16  3e-15\n",
      "25: -4.9569e+02 -1.5070e+03  1e+03  2e-16  3e-16\n",
      "26: -6.5445e+02 -8.6954e+02  2e+02  2e-16  2e-16\n",
      "27: -6.8875e+02 -7.8630e+02  1e+02  1e-16  1e-16\n",
      "28: -6.9093e+02 -7.0950e+02  2e+01  1e-16  4e-17\n",
      "29: -6.9630e+02 -7.1280e+02  2e+01  1e-16  1e-16\n",
      "30: -6.9637e+02 -6.9664e+02  3e-01  2e-16  1e-16\n",
      "31: -6.9638e+02 -6.9638e+02  4e-03  2e-16  1e-16\n",
      "32: -6.9638e+02 -6.9638e+02  4e-05  2e-16  1e-16\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  0.0000e+00 -5.0000e+04  1e+01  1e+00  1e+00\n",
      " 1:  8.4695e+01 -3.1317e+06  4e+02  1e+00  1e+00\n",
      " 2:  1.3736e+02 -7.0162e+06  8e+02  1e+00  1e+00\n",
      " 3:  5.6729e+03 -1.4917e+08  2e+04  1e+00  1e+00\n",
      " 4:  6.0217e+03 -1.5580e+08  2e+04  1e+00  1e+00\n",
      " 5:  7.7288e+03 -1.8552e+08  3e+04  1e+00  1e+00\n",
      " 6:  7.8262e+03 -1.8773e+08  3e+04  1e+00  1e+00\n",
      " 7:  7.9764e+03 -1.9133e+08  3e+04  1e+00  1e+00\n",
      " 8:  8.2691e+03 -1.9846e+08  3e+04  1e+00  1e+00\n",
      " 9:  9.3418e+03 -2.2496e+08  4e+04  1e+00  1e+00\n",
      "10:  1.2246e+04 -3.0288e+08  5e+04  1e+00  1e+00\n",
      "11:  1.4015e+04 -3.8142e+08  7e+04  1e+00  1e+00\n",
      "12:  6.1567e+03 -4.5363e+08  8e+04  1e+00  1e+00\n",
      "13:  7.5813e+03 -7.7937e+08  1e+05  1e+00  1e+00\n",
      "14:  1.0412e+04 -2.3320e+09  5e+05  1e+00  1e+00\n",
      "15:  2.0122e+04 -4.4011e+10  9e+06  1e+00  1e+00\n",
      "16:  1.0323e+04 -5.8085e+12  1e+09  1e+00  1e+00\n",
      "17:  7.5984e+10 -1.3535e+20  1e+20  4e-13  1e-03\n",
      "18:  7.5984e+10 -1.3535e+18  1e+18  4e-15  3e-04\n",
      "19:  7.5984e+10 -1.3540e+16  1e+16  2e-16  4e-06\n",
      "20:  7.5980e+10 -1.4011e+14  1e+14  1e-16  3e-08\n",
      "21:  7.5593e+10 -6.0555e+12  6e+12  1e-16  1e-06\n",
      "22:  9.2620e+09 -5.7620e+12  6e+12  2e-16  1e-06\n",
      "23:  7.9595e+09 -1.0747e+11  1e+11  2e-16  2e-08\n",
      "24:  1.5616e+09 -4.3320e+09  6e+09  2e-16  6e-10\n",
      "25:  2.3088e+08 -2.6581e+08  5e+08  3e-16  5e-12\n",
      "26:  3.3054e+07 -3.6862e+07  7e+07  2e-16  1e-13\n",
      "27:  4.7078e+06 -5.3000e+06  1e+07  3e-16  7e-14\n",
      "28:  6.6221e+05 -7.7064e+05  1e+06  1e-16  8e-15\n",
      "29:  8.9929e+04 -1.1523e+05  2e+05  2e-16  7e-15\n",
      "30:  1.0803e+04 -1.8488e+04  3e+04  1e-16  3e-15\n",
      "31:  5.4357e+02 -3.5517e+03  4e+03  2e-16  4e-16\n",
      "32: -5.6059e+02 -1.0521e+03  5e+02  2e-16  1e-16\n",
      "33: -6.7481e+02 -7.5344e+02  8e+01  9e-17  1e-16\n",
      "34: -6.9083e+02 -7.0721e+02  2e+01  3e-16  5e-17\n",
      "35: -6.9588e+02 -7.0511e+02  9e+00  2e-16  3e-16\n",
      "36: -6.9634e+02 -6.9672e+02  4e-01  2e-16  2e-16\n",
      "37: -6.9641e+02 -6.9645e+02  4e-02  1e-16  7e-17\n",
      "38: -6.9641e+02 -6.9641e+02  2e-03  2e-16  7e-17\n",
      "39: -6.9641e+02 -6.9641e+02  3e-05  1e-16  5e-17\n",
      "Optimal solution found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\florian\\anaconda3\\envs\\aix360\\lib\\site-packages\\cvxopt\\coneprog.py:2111: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if 'x' in initvals:\n",
      "c:\\users\\florian\\anaconda3\\envs\\aix360\\lib\\site-packages\\cvxopt\\coneprog.py:2116: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if 's' in initvals:\n",
      "c:\\users\\florian\\anaconda3\\envs\\aix360\\lib\\site-packages\\cvxopt\\coneprog.py:2131: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if 'y' in initvals:\n",
      "c:\\users\\florian\\anaconda3\\envs\\aix360\\lib\\site-packages\\cvxopt\\coneprog.py:2136: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if 'z' in initvals:\n"
     ]
    }
   ],
   "source": [
    "explainer = ProtodashExplainer()\n",
    "(W, S, setValues) = explainer.explain(X, z_train_good, m=5) # Return weights W, Prototypes S and objective function values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Comparison"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Index 0 = Chosen Sample"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Index 1-5 = Most similiar prototypes"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>33</td>\n",
       "      <td>27</td>\n",
       "      <td>43</td>\n",
       "      <td>20</td>\n",
       "      <td>27</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>priors_count</th>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>days_b_screening_arrest</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>21</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decile_score</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>length_of_stay</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c_charge_degree_F</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c_charge_degree_M</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race_African-American</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race_Asian</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race_Caucasian</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race_Hispanic</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race_Native American</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race_Other</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_cat_25 - 45</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_cat_Greater than 45</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_cat_Less than 25</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_text_High</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_text_Low</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_text_Medium</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sex_Female</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sex_Male</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>two_year_recid</th>\n",
       "      <td>recid</td>\n",
       "      <td>recid</td>\n",
       "      <td>recid</td>\n",
       "      <td>recid</td>\n",
       "      <td>recid</td>\n",
       "      <td>recid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gt_two_year_recid</th>\n",
       "      <td>recid</td>\n",
       "      <td>recid</td>\n",
       "      <td>recid</td>\n",
       "      <td>recid</td>\n",
       "      <td>recid</td>\n",
       "      <td>recid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weight</th>\n",
       "      <td>1</td>\n",
       "      <td>0.810861</td>\n",
       "      <td>0.00897464</td>\n",
       "      <td>0.0984887</td>\n",
       "      <td>0.0568504</td>\n",
       "      <td>0.0248254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             0         1           2          3          4  \\\n",
       "age                         33        27          43         20         27   \n",
       "priors_count                14        12           3          0         27   \n",
       "days_b_screening_arrest     -1        -1          21         -2          0   \n",
       "decile_score                10         8           3         10         10   \n",
       "length_of_stay               1         1           2          0          0   \n",
       "c_charge_degree_F            1         1           0          0          0   \n",
       "c_charge_degree_M            0         0           1          1          1   \n",
       "race_African-American        1         1           0          0          1   \n",
       "race_Asian                   0         0           0          0          0   \n",
       "race_Caucasian               0         0           0          1          0   \n",
       "race_Hispanic                0         0           1          0          0   \n",
       "race_Native American         0         0           0          0          0   \n",
       "race_Other                   0         0           0          0          0   \n",
       "age_cat_25 - 45              1         1           1          0          1   \n",
       "age_cat_Greater than 45      0         0           0          0          0   \n",
       "age_cat_Less than 25         0         0           0          1          0   \n",
       "score_text_High              1         1           0          1          1   \n",
       "score_text_Low               0         0           1          0          0   \n",
       "score_text_Medium            0         0           0          0          0   \n",
       "sex_Female                   0         0           0          0          0   \n",
       "sex_Male                     1         1           1          1          1   \n",
       "two_year_recid           recid     recid       recid      recid      recid   \n",
       "gt_two_year_recid        recid     recid       recid      recid      recid   \n",
       "Weight                       1  0.810861  0.00897464  0.0984887  0.0568504   \n",
       "\n",
       "                                 5  \n",
       "age                             64  \n",
       "priors_count                    13  \n",
       "days_b_screening_arrest         -1  \n",
       "decile_score                     6  \n",
       "length_of_stay                   1  \n",
       "c_charge_degree_F                1  \n",
       "c_charge_degree_M                0  \n",
       "race_African-American            1  \n",
       "race_Asian                       0  \n",
       "race_Caucasian                   0  \n",
       "race_Hispanic                    0  \n",
       "race_Native American             0  \n",
       "race_Other                       0  \n",
       "age_cat_25 - 45                  0  \n",
       "age_cat_Greater than 45          1  \n",
       "age_cat_Less than 25             0  \n",
       "score_text_High                  0  \n",
       "score_text_Low                   0  \n",
       "score_text_Medium                1  \n",
       "sex_Female                       0  \n",
       "sex_Male                         1  \n",
       "two_year_recid               recid  \n",
       "gt_two_year_recid            recid  \n",
       "Weight                   0.0248254  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dfs = pd.DataFrame.from_records(z_train_good[S, :].astype('double'))\n",
    "# make preds and get gt\n",
    "\n",
    "RP=[]\n",
    "GT=[]\n",
    "for i in range(S.shape[0]):\n",
    "    RP.append(class_names[1 if z_train_good[S[i], -1] > 0.5 else 0]) # Append class names\n",
    "    GT.append(class_names[int(y_z_train_good[S[i]])])\n",
    "dfs[21] = RP\n",
    "dfs.columns = cols\n",
    "dfs['gt_two_year_recid'] = GT\n",
    "dfs[\"Weight\"] = np.around(W, 5)/np.sum(np.around(W, 5)) # Calculate normalized importance weights\n",
    "dfs = pd.concat([dfx, dfs])\n",
    "dfs.reset_index(inplace=True, drop=True)\n",
    "display(Markdown(\"#### Comparison\"))\n",
    "display(Markdown(\"Index 0 = Chosen Sample\"))\n",
    "display(Markdown(\"Index 1-5 = Most similiar prototypes\"))\n",
    "display(dfs.transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.B. AI Fairness 360\n",
    "\n",
    "For bias mitigation there is an additional package next to [AI Explainability 360](https://aif360.mybluemix.net/). AI Fairness 360 comes includes 10 algorithms that can be used in pre-, in- or post-processing.\n",
    "\n",
    "\n",
    "In the package there are some datasets already included. To use the algorithms you need to transform you data into a [Structured Dataset](https://aif360.readthedocs.io/en/latest/modules/datasets.html#structured-dataset). Luckily the COMPAS dataset is already available via the package and we can skip this step. First we load the COMPAS dataset and define our privileged and unprivileged groups. In this case we are sensible that African-Americans get discriminated from our model and Caucasians favored. So we set African-Americans as the unprivileged and Caucasians as the privileged group.\n",
    "\n",
    "After that we evaluate the bias with the already included [BinaryLabelDatasetMetric](https://aif360.readthedocs.io/en/latest/modules/metrics.html#aif360.metrics.BinaryLabelDatasetMetric) the difference in the mean outcomes regarding these groups. It seems there is quite a big difference around 10% between the mean outcomes of African-Americans and Caucasians. Therefor we use one of 10 metrics from AI Fairness [Reweighing](https://aif360.readthedocs.io/en/latest/modules/preprocessing.html#aif360.algorithms.preprocessing.Reweighing). Reweighing is a preprocesing step to mitigate bias. To use this algorithm we need to have the dataset defined as a [Structured Dataset](https://aif360.readthedocs.io/en/latest/modules/datasets.html#structured-dataset) as the weights of the different feature attributes will be adjusted. After executing reweighing, we test the difference in the mean outcomes between our groups again and find that the bias has been completely eliminated.\n",
    "\n",
    "Note that this works perfectly if we test the exact same data again, but if there is a dataset shift, we would need to adjust the weights again for complete bias reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Missing Data: 5 rows removed from CompasDataset.\n",
      "c:\\users\\florian\\anaconda3\\envs\\aix360\\lib\\site-packages\\aif360\\datasets\\standard_dataset.py:121: FutureWarning: outer method for ufunc <ufunc 'equal'> is not implemented on pandas objects. Returning an ndarray, but in the future this will raise a 'NotImplementedError'. Consider explicitly converting the Series to an array with '.array' first.\n",
      "  priv = np.logical_or.reduce(np.equal.outer(vals, df[attr]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "aif360.datasets.compas_dataset.CompasDataset"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_orig = CompasDataset()\n",
    "\n",
    "dataset_orig_train, dataset_orig_test = dataset_orig.split([0.7], shuffle=True)\n",
    "\n",
    "privileged_groups = [{'race': 1}] # African-American\n",
    "unprivileged_groups = [{'race': 0}] # Caucasian\n",
    "type(dataset_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Original training dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = -0.119411\n"
     ]
    }
   ],
   "source": [
    "metric_orig_train = BinaryLabelDatasetMetric(dataset_orig_train, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Original training dataset\"))\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_train.mean_difference())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "RW = Reweighing(unprivileged_groups=unprivileged_groups,\n",
    "                privileged_groups=privileged_groups)\n",
    "dataset_transf_train = RW.fit_transform(dataset_orig_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Transformed training dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = 0.000000\n"
     ]
    }
   ],
   "source": [
    "metric_transf_train = BinaryLabelDatasetMetric(dataset_transf_train, \n",
    "                                               unprivileged_groups=unprivileged_groups,\n",
    "                                               privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Transformed training dataset\"))\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_transf_train.mean_difference())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "Both open source tool kits provide multiple algorithms for visualization, bias mitigation and analyses of individual samples. WIT offers a graphical interface and is easy to learn for non-technical and technical users. On the other side AIF360/AIX360 lacks in those areas, but instead has a growing number of algorithms based on recent published papers. Further is AIF360/AIX360 a library and hence quite more flexible as we can call each function directly and don't have to interact via a graphical user interface like in WIT. \n",
    "\n",
    "In our conclusion there is no winner as both tools have their own user base. If you don't need to be an expert in explaining the functionality of your model and only want to have a rough overview about biases and your data distribution WIT is sufficient. On the other side if the priority of your project is to eliminate any discrimination of your model AIF360/AIX360 offers you more functionalities and comes with a bunch of scientific papers that can support your decision making.\n",
    "\n",
    "Nevertheless, we as model developers should be more sensible about discriminating models and be able to explain how they make predictions or what their weaknesses/disadvantages are. This requires that we understand our data and model for which WIT and AIF360/AIX360 offers us a reasonable framework."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
